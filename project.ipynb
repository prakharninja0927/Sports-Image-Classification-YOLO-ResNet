{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Image Classification\n",
    "Dataset: https://www.kaggle.com/code/littlebughenrylee/100-sports-classification-resnet-93-yolo-98\n",
    "\n",
    "<p>Collection of sports images covering 100 different sports.. Images are 224,224,3 jpg format. Data is separated into train, test and valid directories. Additionallly a csv file is included for those that wish to use it to create there own train, test and validation datasets.</p>\n",
    "\n",
    "Images were gathered from internet searches. The images were scanned with a duplicate image detector program I wrote. Any duplicate images were removed to prevent bleed through of images between the train, test and valid data sets. All images were then resized to 224 X224 X 3 and converted to jpg format. A csv file is included that for each image file contains the relative path to the image file, the image file class label and the dataset (train, test or valid) that the image file resides in. This is a clean dataset.\n",
    "\n",
    "<img src=\"https://t3.ftcdn.net/jpg/02/78/42/76/360_F_278427683_zeS9ihPAO61QhHqdU1fOaPk2UClfgPcW.jpg\" class=\"center\">\n",
    "<style>.center {\n",
    "  display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "import kaggle\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import glob\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset from Kaggle\n",
    "link: https://www.kaggle.com/datasets/gpiosenka/sports-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Already Exist\n"
     ]
    }
   ],
   "source": [
    "if \"sports-classification\" in os.listdir():\n",
    "    print('Data Already Exist')\n",
    "else:\n",
    "  print(\"Downloading Data From Kaggle\")\n",
    "  !kaggle datasets download -d gpiosenka/sports-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Downloaded ZIP File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\sports-classification.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the filenames without extension\n",
    "data_folder_name = glob.glob(os.path.join(directory_path, '*.zip'))\n",
    "filenames_without_extension = [os.path.splitext(os.path.basename(file))[0] for file in data_folder_name]\n",
    "data_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_name = filenames_without_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_name = data_folder_name[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine If Dataset is unzipped or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'sports-classification' already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_folder_name):\n",
    "    os.makedirs(data_folder_name)\n",
    "    print(f\"Directory '{data_folder_name}' created.\")\n",
    "    !unzip sports-classification.zip -d 'sports-classification/'\n",
    "else:\n",
    "    print(f\"Directory '{data_folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS_DIR = 'models_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'models_results' already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_RESULTS_DIR):\n",
    "    os.makedirs(MODEL_RESULTS_DIR)\n",
    "    print(f\"Directory '{MODEL_RESULTS_DIR}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{MODEL_RESULTS_DIR}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and get total number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sports-classification/sports.csv')\n",
    "NUM_CLASSES = data['class id'].nunique()\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Device either GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# hardcode :\n",
    "# loss_fn -> CrossEntropyLoss\n",
    "# optimizer -> Adam(lr = 0.0005)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ImageDataset\n",
    "The ImageDataset class is a custom dataset implementation for handling image data in PyTorch. It is designed to work with a specific directory structure where each sub-directory corresponds to a distinct class, and the images belonging to that class are stored within that sub-directory. This structure is commonly used in image classification datasets.\n",
    "\n",
    "The class supports image data in the JPG format. \n",
    "\n",
    "Attributes: The class has attributes root_dir, transform, images, labels, and class_labels.\n",
    "\n",
    "- `root_dir`: The root directory path where the image data is stored.\n",
    "- `transform`: A function for image transformations (e.g., resizing, normalization).\n",
    "- `images`: A list to store the file paths of all the images in the dataset.\n",
    "- `labels`: A list to store the corresponding class labels (as integers) for each image.\n",
    "- `class_labels`: A dictionary mapping class directory names to integer labels.\n",
    "\n",
    "`__len__` Method: Implements the `__len__` method to return the total number of samples (images) in the dataset. It returns the length of the images list.\n",
    "\n",
    "`__getitem__` Method: Implements the `__getitem__` method to access a specific sample (image) and its associated label by index. It loads the image from the file path using PIL, applies the specified transformation (if any), and returns the transformed image along with its label.\n",
    "\n",
    "Overall, the ImageDataset class simplifies the process of working with image datasets in PyTorch, providing a flexible and robust solution for handling image data in machine learning projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the ImageDataset object.\n",
    "\n",
    "        Parameters:\n",
    "            root_dir (str): The root directory path containing sub-directories, each representing a class.\n",
    "            transform (optional, callable): A callable function to transform the images (e.g., resizing, normalization).\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_labels = {}\n",
    "\n",
    "        # Create a mapping of class labels to integers\n",
    "        self.class_labels = {}\n",
    "        class_idx = 0\n",
    "\n",
    "        # Iterate over sub-directories\n",
    "        for class_dir in os.listdir(self.root_dir):\n",
    "            class_dir_path = os.path.join(self.root_dir, class_dir)\n",
    "            if os.path.isdir(class_dir_path):\n",
    "                self.class_labels[class_dir] = class_idx\n",
    "                class_idx += 1\n",
    "\n",
    "                # Iterate over images in the sub-directory\n",
    "                for img_filename in os.listdir(class_dir_path):\n",
    "                    if img_filename.endswith(\".jpg\"):\n",
    "                        img_path = os.path.join(class_dir_path, img_filename)\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_labels[class_dir])\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image.Image: The image sample.\n",
    "            int: The label associated with the image.\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[idx])        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if image.mode == \"L\":\n",
    "            image = Image.merge(\"RGB\", (image, image, image))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader Function for Image Classification\n",
    "\n",
    "### Description:\n",
    "The `model_dataloader` function is designed to create PyTorch data loaders for training, validation, and testing datasets in an image classification task. It uses a custom dataset, named `ImageDataset`, which is assumed to handle a specific directory structure where each sub-directory represents a distinct class, and the images belonging to that class are stored within the respective sub-directory. This function takes weights and a transformation function as input and returns three PyTorch data loaders for the training, validation, and testing datasets.\n",
    "\n",
    "### Parameters:\n",
    "- `weights`: A variable (or value) that represents the weights used in the dataset (this variable is defined outside the function).\n",
    "- `transform`: A transformation function for image preprocessing, data augmentation, or resizing. The `transform` function is applied to the images loaded from the dataset.\n",
    "\n",
    "### Data Folder Structure:\n",
    "- The function assumes that the image data is stored in the \"sports-classification\" folder in the following structure:\n",
    "\n",
    "        sports-classification/ <br>\n",
    "        ├── train/ <br>\n",
    "        ├── valid/ <br>\n",
    "        └── test/ <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataloader\n",
    "def model_dataloder(weights, transform):\n",
    "    \"\"\"\n",
    "    Returns three PyTorch DataLoaders for training, validation, and testing.\n",
    "    \n",
    "    Parameters:\n",
    "        weights (list): A list of weights used for data sampling in DataLoader (optional).\n",
    "        transform (torchvision.transforms): Image transformation to be applied to the datasets.\n",
    "        \n",
    "    Returns:\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        val_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        test_dataloader (DataLoader): DataLoader for the test dataset.\n",
    "    \"\"\"\n",
    "    weights = weights\n",
    "    \n",
    "    data_folder = \"sports-classification\"\n",
    "\n",
    "    train_folder = data_folder + \"/train\"\n",
    "    val_folder = data_folder + \"/valid\"\n",
    "    test_folder = data_folder + \"/test\"\n",
    "\n",
    "    # Images from internet\n",
    "    image_urls = \"testing_images\"\n",
    "    \n",
    "    # pytorch dataset\n",
    "    train_dataset = ImageDataset(train_folder, transform = transform)\n",
    "    val_dataset = ImageDataset(val_folder, transform = transform)\n",
    "    test_dataset = ImageDataset(test_folder, transform = transform)\n",
    "\n",
    "    # test downloaded  images \n",
    "    custom_images = ImageDataset(image_urls, transform = transform)\n",
    "    \n",
    "    # pytorch dataloader\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = 32, shuffle = True)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size = 32, shuffle = False)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = 32, shuffle = False)\n",
    "    custom_dataloader = DataLoader(dataset = custom_images, batch_size = 32, shuffle = False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "### Description:\n",
    "The `train` function is used to perform one training epoch for a given model using a specified data loader. It computes the training loss and accuracy during the training process.\n",
    "\n",
    "### Parameters:\n",
    "- `model`: The PyTorch model to be trained.\n",
    "- `dataloader`: The PyTorch data loader containing the training dataset.\n",
    "- `loss_fn`: The loss function used to compute the training loss.\n",
    "- `optimizer`: The optimizer used to update the model's parameters during training.\n",
    "- `device`: The device on which to perform computations (e.g., \"cuda\" for GPU or \"cpu\" for CPU).\n",
    "\n",
    "### Returns:\n",
    "- `train_loss`: The average training loss computed over the training dataset.\n",
    "- `train_acc`: The average training accuracy computed over the training dataset.\n",
    "\n",
    "## Validation Function\n",
    "\n",
    "### Description:\n",
    "The `val` function is used to evaluate a trained model on a validation dataset. It computes the validation loss and accuracy.\n",
    "\n",
    "### Parameters:\n",
    "- `model`: The PyTorch model to be evaluated.\n",
    "- `dataloader`: The PyTorch data loader containing the validation dataset.\n",
    "- `loss_fn`: The loss function used to compute the validation loss.\n",
    "- `device`: The device on which to perform computations (e.g., \"cuda\" for GPU or \"cpu\" for CPU).\n",
    "\n",
    "### Returns:\n",
    "- `val_loss`: The average validation loss computed over the validation dataset.\n",
    "- `val_acc`: The average validation accuracy computed over the validation dataset.\n",
    "\n",
    "## Test Accuracy Function\n",
    "\n",
    "### Description:\n",
    "The `test_accuracy_resnet` function is used to evaluate the accuracy of a trained model on a test dataset. It computes the accuracy of the model's predictions compared to the actual labels.\n",
    "\n",
    "### Parameters:\n",
    "- `model`: The PyTorch model to be evaluated.\n",
    "- `dataloader`: The PyTorch data loader containing the test dataset.\n",
    "- `device`: The device on which to perform computations (e.g., \"cuda\" for GPU or \"cpu\" for CPU).\n",
    "\n",
    "### Returns:\n",
    "- `accuracy`: The accuracy of the model's predictions on the test dataset, represented as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> train_loss, train_acc\n",
    "def train (model, dataloader, loss_fn, optimizer, device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (x, y) in enumerate (dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        train_pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(train_pred, y)\n",
    "        train_loss = train_loss + loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_pred_label = torch.argmax(torch.softmax(train_pred, dim = 1), dim = 1)\n",
    "        train_acc = train_acc + (train_pred_label == y).sum().item() / len(train_pred)\n",
    "    \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "# Val -> val_loss, val_acc\n",
    "def val (model, dataloader, loss_fn, device):\n",
    "    val_loss, val_acc = 0, 0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            val_pred = model(x)\n",
    "            \n",
    "            loss = loss_fn(val_pred, y)\n",
    "            val_loss = val_loss + loss.item()\n",
    "            \n",
    "            val_pred_label = torch.argmax(torch.softmax(val_pred, dim = 1), dim = 1)\n",
    "            val_acc = val_acc + (val_pred_label == y).sum().item() / len(val_pred)\n",
    "        \n",
    "        val_loss = val_loss / len(dataloader)\n",
    "        val_acc = val_acc / len(dataloader)\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "def test_accuracy_resnet(model,dataloader,device):\n",
    "    # empty list store labels\n",
    "    predict_label_list = []\n",
    "    actual_label_list = []\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in dataloader: \n",
    "        \n",
    "        for label in labels:\n",
    "            label = label.item()\n",
    "            actual_label_list.append(label)\n",
    "        \n",
    "        for image in images:\n",
    "            with torch.inference_mode():\n",
    "                image = image.to(device)\n",
    "                # add batch_size and device\n",
    "                image = image.unsqueeze(dim = 0)\n",
    "                # logits\n",
    "                logits = model(image)\n",
    "                # lables\n",
    "                label = torch.argmax(logits).item()\n",
    "                print(label)\n",
    "                predict_label_list.append(label)\n",
    "\n",
    "    accuracy = accuracy_score(actual_label_list, predict_label_list)\n",
    "    return accuracy*100\n",
    "\n",
    "def classify_custom_images(model,dataloader,device,df):\n",
    "    \n",
    "    pred_labels = []\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in dataloader: \n",
    "        \n",
    "        for image in images:\n",
    "            with torch.inference_mode():\n",
    "                image = image.to(device)\n",
    "                # add batch_size and device\n",
    "                image = image.unsqueeze(dim = 0)\n",
    "                # logits\n",
    "                logits = model(image)\n",
    "                # lables\n",
    "                label = torch.argmax(logits).item()\n",
    "                text_label = df[df['class id']==label]['labels'].iloc[0]\n",
    "                pred_labels.append(text_label)\n",
    "    return pred_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Function\n",
    "\n",
    "### Description:\n",
    "The `training_loop` function is responsible for training a given PyTorch model on a training dataset and evaluating its performance on a validation dataset. It allows you to control the number of training epochs and implements early stopping based on the `patience` parameter. The function records and returns the training and validation losses, accuracies, and training time for each epoch. Additionally, it displays two line charts showing the loss and accuracy trends over epochs.\n",
    "\n",
    "### Parameters:\n",
    "- `model`: The PyTorch model to be trained and evaluated.\n",
    "- `train_dataloader`: The PyTorch data loader containing the training dataset.\n",
    "- `val_dataloader`: The PyTorch data loader containing the validation dataset.\n",
    "- `device`: The device on which to perform computations (e.g., \"cuda\" for GPU or \"cpu\" for CPU).\n",
    "- `epochs`: The number of epochs for training.\n",
    "- `patience`: The number of consecutive epochs with no improvement in validation loss to trigger early stopping.\n",
    "\n",
    "### Returns:\n",
    "- `model_results`: A pandas DataFrame containing the results for each epoch, including training loss, training accuracy, validation loss, validation accuracy, and time taken for each epoch.\n",
    "- `training_time`: The total time taken for the entire training process.\n",
    "\n",
    "### Training and Early Stopping:\n",
    "The function trains the model for the specified number of epochs. If the validation loss does not improve for `patience` consecutive epochs, early stopping is triggered, and the training process stops to avoid overfitting.\n",
    "\n",
    "### Line Charts:\n",
    "The function creates two line charts using Plotly to visualize the loss and accuracy trends over epochs for both training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, val_dataloader, device, epochs, patience):\n",
    "    # empty dict for restore results\n",
    "    results = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "    \n",
    "    # hardcode loss_fn and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "\n",
    "    # variable to hold the training time\n",
    "    training_time = 0.0\n",
    "    epoch_run_time = []\n",
    "    # loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "         # record the start time for each epoch\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model = model, \n",
    "                                      dataloader = train_dataloader,\n",
    "                                      loss_fn = loss_fn,\n",
    "                                      optimizer = optimizer,\n",
    "                                      device = device)\n",
    "        \n",
    "        val_loss, val_acc = val(model = model,\n",
    "                                dataloader = val_dataloader,\n",
    "                                loss_fn = loss_fn,\n",
    "                                device = device)\n",
    "        \n",
    "        # record the end time for each epoch\n",
    "        epoch_end_time = time.time()\n",
    "        \n",
    "        # calculate the time taken for this epoch\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        epoch_run_time.append(epoch_time)\n",
    "        training_time += epoch_time\n",
    "        \n",
    "        # print results for each epoch\n",
    "        print(f\"Epoch: {epoch+1}\\n\"\n",
    "              f\"Train loss: {train_loss:.4f} | Train accuracy: {(train_acc*100):.3f}%\\n\"\n",
    "              f\"Val loss: {val_loss:.4f} | Val accuracy: {(val_acc*100):.3f}%\\n\"\n",
    "              f\"| Epoch time: {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # record results for each epoch\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        \n",
    "        # calculate average \"val_loss\" for early_stopping\n",
    "        mean_val_loss = np.mean(results[\"val_loss\"])\n",
    "        best_val_loss = float(\"inf\")\n",
    "        num_no_improvement = 0\n",
    "        if np.mean(mean_val_loss > best_val_loss):\n",
    "            best_val_loss = mean_val_loss\n",
    "        \n",
    "            model_state_dict = model.state_dict()\n",
    "            best_model.load_state_dict(model_state_dict)\n",
    "        else:\n",
    "            num_no_improvement +=1\n",
    "    \n",
    "        if num_no_improvement == patience:\n",
    "            break\n",
    "    \n",
    "    # Saving Results for model\n",
    "    dic = {\n",
    "        \"epochs\": list(range(1,epochs+1)),\n",
    "        'Train_loss':results[\"train_loss\"],\n",
    "        'Train_Accuracy': results['train_acc'],\n",
    "        'Validation_loss':results[\"val_loss\"],\n",
    "        'Validation_Accuracy': results['val_acc'],\n",
    "        'Time_Taken':epoch_run_time\n",
    "\n",
    "    }\n",
    "    model_results = pd.DataFrame(dic)\n",
    "    \n",
    "\n",
    "    # Create the figure for the chart\n",
    "    fig = go.Figure()\n",
    "        # Add the 'Train loss' and 'Val loss' traces as lines\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(results[\"train_loss\"]) + 1)), \n",
    "                            y=results[\"train_loss\"], mode='lines', name='Train loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(results[\"val_loss\"]) + 1)), \n",
    "                            y=results[\"val_loss\"], mode='lines', name='Val loss'))\n",
    "    \n",
    "\n",
    "    # Update the layout for better visualization\n",
    "    fig.update_layout(title=\"Loss over Epochs\",\n",
    "                    xaxis_title=\"Epochs\",\n",
    "                    yaxis_title=\"Loss\",\n",
    "                    legend=dict(x=0.05, y=1.1),\n",
    "                    width=800, height=400)\n",
    "    \n",
    "    # Create the figure for the chart\n",
    "    fig2 = go.Figure()\n",
    "        # Add the 'Train loss' and 'Val loss' traces as lines\n",
    "    fig2.add_trace(go.Scatter(x=list(range(1, len(results[\"train_acc\"]) + 1)), \n",
    "                            y=results[\"train_acc\"], mode='lines', name='Train Accuracy'))\n",
    "    fig2.add_trace(go.Scatter(x=list(range(1, len(results[\"val_acc\"]) + 1)), \n",
    "                            y=results[\"val_acc\"], mode='lines', name='Val Accuracy'))\n",
    "    \n",
    "\n",
    "    # Update the layout for better visualization\n",
    "    fig2.update_layout(title=\"Accuracy over Epochs\",\n",
    "                    xaxis_title=\"Epochs\",\n",
    "                    yaxis_title=\"Accuracy\",\n",
    "                    legend=dict(x=0.05, y=1.1),\n",
    "                    width=800, height=400)\n",
    "\n",
    "    # combined_fig.show()\n",
    "    fig.show()\n",
    "    fig2.show()\n",
    "    \n",
    "    return model_results, training_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
